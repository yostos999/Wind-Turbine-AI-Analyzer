import json
import time
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
import traceback

from config import get_current_llm_config, LLM_PROVIDER
from utils import logger, error_handler

# =============================================================================
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
# =============================================================================

PROMPT_TEMPLATES = {
    "data_summary": """
ë‹¹ì‹ ì€ í’ë ¥ ë°œì „ì†Œì˜ ì„¤ë¹„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš´ì˜ í˜„í™©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

## ë¶„ì„ ë°ì´í„°:
{analysis_data}

ë‹¤ìŒ ê´€ì ì—ì„œ ìš”ì•½í•´ì£¼ì„¸ìš”:
1. **ë°ì´í„° ê°œìš”**: ë¶„ì„ ê¸°ê°„, ë°ì´í„° í’ˆì§ˆ, ì£¼ìš” íŠ¹ì§•
2. **ë°œì „ ì„±ëŠ¥**: ìš©ëŸ‰ê³„ìˆ˜, ê°€ë™ë¥ , í‰ê·  ë°œì „ëŸ‰
3. **ìš´ì˜ íš¨ìœ¨**: ì •ìƒ ìš´ì˜ ë¹„ìœ¨, ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ
4. **í•µì‹¬ ë°œê²¬ì‚¬í•­**: ê°€ì¥ ì¤‘ìš”í•œ 3ê°€ì§€ í¬ì¸íŠ¸

ì „ë¬¸ì ì´ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
""",

    "performance_analysis": """
ë‹¹ì‹ ì€ í’ë ¥ í„°ë¹ˆ ì„±ëŠ¥ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì„±ëŠ¥ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

## ì„±ëŠ¥ ë¶„ì„ ë°ì´í„°:
{analysis_data}

ë‹¤ìŒ í•­ëª©ì— ëŒ€í•´ ìƒì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. **ì„±ëŠ¥ ê³¡ì„  ë¶„ì„**: í’ì†ë³„ ë°œì „ íš¨ìœ¨, ìµœì  ìš´ì˜ êµ¬ê°„
2. **ë¹„íš¨ìœ¨ êµ¬ê°„ ë¶„ì„**: ì„±ëŠ¥ ì €í•˜ ì›ì¸, ë°œìƒ íŒ¨í„´
3. **í™˜ê²½ ìš”ì¸ ì˜í–¥**: ì˜¨ë„, ìŠµë„, í’í–¥ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
4. **ìƒê´€ê´€ê³„ ë¶„ì„**: ì£¼ìš” ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ í•´ì„

ì„¤ë¹„íŒ€ì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
""",

    "efficiency_diagnosis": """
ë‹¹ì‹ ì€ ì„¤ë¹„ íš¨ìœ¨ì„± ì§„ë‹¨ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¹„íš¨ìœ¨ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì œì ì„ ì§„ë‹¨í•´ì£¼ì„¸ìš”.

## ë¹„íš¨ìœ¨ ë¶„ì„ ë°ì´í„°:
{analysis_data}

ë‹¤ìŒ ê´€ì ì—ì„œ ì§„ë‹¨í•´ì£¼ì„¸ìš”:
1. **ë¹„íš¨ìœ¨ í˜„í™©**: ì „ì²´ ëŒ€ë¹„ ë¹„íš¨ìœ¨ ë¹„ìœ¨, ì‹¬ê°ë„ í‰ê°€
2. **íŒ¨í„´ ë¶„ì„**: ì‹œê°„ëŒ€ë³„, í’ì†ë³„ ë¹„íš¨ìœ¨ ë°œìƒ íŒ¨í„´
3. **ê·¼ë³¸ ì›ì¸**: ì˜ˆìƒë˜ëŠ” ë¹„íš¨ìœ¨ ë°œìƒ ì›ì¸ë“¤
4. **ì˜í–¥ë„ í‰ê°€**: ë°œì „ëŸ‰ ì†ì‹¤, ìˆ˜ìµì„± ì˜í–¥

ì •ë¹„íŒ€ì´ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.
""",

    "improvement_recommendations": """
ë‹¹ì‹ ì€ í’ë ¥ ë°œì „ì†Œ ìš´ì˜ ìµœì í™” ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ì¢…í•© ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.

## ì¢…í•© ë¶„ì„ ê²°ê³¼:
{analysis_data}

ë‹¤ìŒ í˜•íƒœë¡œ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”:
1. **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„ ì•ˆ** (1-2ì£¼ ë‚´ ì‹¤í–‰)
2. **ë‹¨ê¸° ê°œì„ ì•ˆ** (1-3ê°œì›” ë‚´ ì‹¤í–‰)  
3. **ì¤‘ì¥ê¸° ê°œì„ ì•ˆ** (6ê°œì›”-1ë…„ ë‚´ ì‹¤í–‰)
4. **íˆ¬ì ëŒ€ë¹„ íš¨ê³¼ ë¶„ì„**: ê° ê°œì„ ì•ˆì˜ ì˜ˆìƒ íš¨ê³¼

ê° ê°œì„ ì•ˆì—ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
- êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ë²•
- ì˜ˆìƒ ë¹„ìš© ë° ê¸°ê°„
- ê¸°ëŒ€ íš¨ê³¼ (ë°œì „ëŸ‰ ì¦ê°€, ë¹„ìš© ì ˆê° ë“±)
- ì‹¤í–‰ ì‹œ ì£¼ì˜ì‚¬í•­

ì‹¤ë¬´ì§„ì´ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
""",

    "comprehensive_report": """
ë‹¹ì‹ ì€ í’ë ¥ ë°œì „ì†Œ ìš´ì˜ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì „ì²´ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ì˜ì§„ ë³´ê³ ìš© ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

## ì „ì²´ ë¶„ì„ ë°ì´í„°:
{analysis_data}

ë‹¤ìŒ êµ¬ì¡°ë¡œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

### ğŸ“Š ìš´ì˜ í˜„í™© ìš”ì•½
- í•µì‹¬ KPI (ìš©ëŸ‰ê³„ìˆ˜, ê°€ë™ë¥ , ë°œì „ëŸ‰)

### ğŸ” ì£¼ìš” ë°œê²¬ì‚¬í•­
- ì„±ëŠ¥ ìš°ìˆ˜ êµ¬ê°„ê³¼ ë¬¸ì œ êµ¬ê°„
- ë¹„íš¨ìœ¨ ë°œìƒ í˜„í™© ë° ì›ì¸

### ğŸ’¡ ê°œì„  ê¸°íšŒ
- ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥í•œ í•­ëª©ë“¤
- ì˜ˆìƒ ê°œì„  íš¨ê³¼ (ì •ëŸ‰ì )

### ğŸ¯ ê¶Œì¥ ì•¡ì…˜
- ìš°ì„ ìˆœìœ„ë³„ ì‹¤í–‰ ê³„íš
- í•„ìš” ìì› ë° ì˜ˆì‚°

### ğŸ“ˆ í–¥í›„ ì „ë§
- ì˜ˆì¸¡ ëª¨ë¸ ê¸°ë°˜ ì„±ê³¼ ì „ë§
- ì§€ì†ì  ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸

ê²½ì˜ì§„ê³¼ ê¸°ìˆ ì§„ ëª¨ë‘ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
}

# =============================================================================
# ì¶”ìƒ ê¸°ë³¸ í´ë˜ìŠ¤
# =============================================================================

class LLMInterface(ABC):
    """LLM ì¸í„°í˜ì´ìŠ¤ ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model = None
        self._initialize_model()
    
    @abstractmethod
    def _initialize_model(self):
        """ëª¨ë¸ ì´ˆê¸°í™” (ê° LLMë³„ êµ¬í˜„)"""
        pass
    
    @abstractmethod
    def _generate_text(self, prompt: str) -> str:
        """í…ìŠ¤íŠ¸ ìƒì„± (ê° LLMë³„ êµ¬í˜„)"""
        pass
    
    def generate_insight(self, analysis_data: Dict[str, Any], 
                        insight_type: str = "comprehensive_report") -> Dict[str, Any]:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_prompt(analysis_data, insight_type)
            
            # LLM í˜¸ì¶œ
            response = self._generate_text(prompt)
            
            return {
                "success": True,
                "insight_type": insight_type,
                "content": response,
                "timestamp": time.time(),
                "model_info": {
                    "provider": LLM_PROVIDER,
                    "model_name": self.config.get("model_name", "unknown")
                }
            }
            
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "insight_type": insight_type,
                "timestamp": time.time()
            }
    
    def _create_prompt(self, analysis_data: Dict[str, Any], insight_type: str) -> str:
        """í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if insight_type not in PROMPT_TEMPLATES:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¸ì‚¬ì´íŠ¸ íƒ€ì…: {insight_type}")
        
        # ë¶„ì„ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ ì •ë¦¬
        data_summary = self._format_analysis_data(analysis_data)
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ë°ì´í„° ì‚½ì…
        prompt = PROMPT_TEMPLATES[insight_type].format(analysis_data=data_summary)
        
        return prompt
    
    def _format_analysis_data(self, data: Dict[str, Any]) -> str:
        """ë¶„ì„ ë°ì´í„°ë¥¼ LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…"""
        formatted_sections = []
        
        # ê¸°ë³¸ í†µê³„
        if "basic_stats" in data:
            basic_info = data["basic_stats"]
            formatted_sections.append(f"""
### ê¸°ë³¸ ë°ì´í„° ì •ë³´:
- ì´ ë°ì´í„° í¬ì¸íŠ¸: {basic_info.get('data_info', {}).get('processed_rows', 'N/A')}ê°œ
- ìš©ëŸ‰ê³„ìˆ˜: {basic_info.get('power_analysis', {}).get('capacity_factor', 0):.3f}
- ë¬´ë°œì „ ë¹„ìœ¨: {basic_info.get('power_analysis', {}).get('zero_power_ratio', 0):.3f}
- ìµœëŒ€ ë°œì „ëŸ‰: {basic_info.get('power_analysis', {}).get('max_power', 'N/A')} kW
""")
        
        # ì„±ëŠ¥ ê³¡ì„ 
        if "performance_curve" in data:
            perf_data = data["performance_curve"]
            formatted_sections.append(f"""
### ì„±ëŠ¥ ê³¡ì„  ë¶„ì„:
- í’ì† êµ¬ê°„ í¬ê¸°: {perf_data.get('bin_size', 'N/A')} m/s
- ë¶„ì„ëœ í’ì† êµ¬ê°„ ìˆ˜: {len(perf_data.get('power_curve_data', []))}ê°œ
""")
        
        # ë¹„íš¨ìœ¨ ë¶„ì„
        if "inefficiency_analysis" in data:
            ineff_data = data["inefficiency_analysis"]
            formatted_sections.append(f"""
### ë¹„íš¨ìœ¨ ë¶„ì„:
- ë¹„íš¨ìœ¨ ë°œìƒë¥ : {ineff_data.get('inefficiency_percentage', 0):.2f}%
- ì´ ìš´ì˜ ë°ì´í„°: {ineff_data.get('total_operating_points', 'N/A')}ê°œ
- ë¹„íš¨ìœ¨ ë°ì´í„°: {ineff_data.get('inefficient_points', 'N/A')}ê°œ
- í‰ê·  ë°œì „ ì†ì‹¤: {ineff_data.get('inefficient_data_summary', {}).get('avg_power_loss', 'N/A')} kW
""")
        
        # ìƒê´€ê´€ê³„ ë¶„ì„
        if "correlation_analysis" in data:
            corr_data = data["correlation_analysis"]
            strong_corr = corr_data.get("strong_correlations", {})
            formatted_sections.append(f"""
### ìƒê´€ê´€ê³„ ë¶„ì„:
- ë°œì „ëŸ‰ê³¼ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ë³€ìˆ˜: {len(strong_corr)}ê°œ
- ìµœê³  ì–‘ì˜ ìƒê´€ê´€ê³„: {corr_data.get('top_positive_correlation', 'N/A')}
- ìµœê³  ìŒì˜ ìƒê´€ê´€ê³„: {corr_data.get('top_negative_correlation', 'N/A')}
""")
        
        # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸
        if "ml_models" in data:
            ml_data = data["ml_models"]
            best_model = ml_data.get("best_model", {})
            formatted_sections.append(f"""
### ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥:
- ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model.get('model_name', 'N/A')}
- RÂ² ì ìˆ˜: {best_model.get('r2_score', 0):.4f}
- í‰ê·  ì ˆëŒ€ ì˜¤ì°¨: {best_model.get('mae', 0):.4f} kW
- í›ˆë ¨ ë°ì´í„°: {ml_data.get('data_split', {}).get('train_size', 'N/A')}ê°œ
""")
        
        # ì¢…í•© ì¸ì‚¬ì´íŠ¸
        if "insights" in data:
            insights_data = data["insights"]
            formatted_sections.append(f"""
### ì¢…í•© ì¸ì‚¬ì´íŠ¸:
- ê¶Œì¥ì‚¬í•­ ìˆ˜: {len(insights_data.get('recommendations', []))}ê°œ
- ì˜ˆì¸¡ ì‹ ë¢°ë„: {insights_data.get('predictive_insights', {}).get('prediction_reliability', 0):.3f}
""")
        
        return "\n".join(formatted_sections)

# =============================================================================
# Gemini êµ¬í˜„
# =============================================================================

class GeminiInterface(LLMInterface):
    """Google Gemini API ì¸í„°í˜ì´ìŠ¤"""
    
    def _initialize_model(self):
        """Gemini ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            import google.generativeai as genai
            
            # API í‚¤ ì„¤ì •
            genai.configure(api_key=self.config["api_key"])
            
            # ëª¨ë¸ ìƒì„±
            generation_config = {
                "temperature": self.config.get("temperature", 0.3),
                "top_p": self.config.get("top_p", 0.8),
                "top_k": self.config.get("top_k", 40),
                "max_output_tokens": self.config.get("max_tokens", 2048),
            }
            
            self.model = genai.GenerativeModel(
                model_name=self.config["model_name"],
                generation_config=generation_config
            )
            
            logger.info(f"Gemini ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.config['model_name']}")
            
        except ImportError:
            raise ImportError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install google-generativeai")
        except Exception as e:
            logger.error(f"Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _generate_text(self, prompt: str) -> str:
        """Gemini APIë¡œ í…ìŠ¤íŠ¸ ìƒì„±"""
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = self.model.generate_content(prompt)
                
                if response.text:
                    return response.text
                else:
                    raise ValueError("API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.warning(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {str(e)}")
                
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    raise

# =============================================================================
# OpenAI êµ¬í˜„ (ì¶”í›„ í™•ì¥ìš©)
# =============================================================================

class OpenAIInterface(LLMInterface):
    """OpenAI API ì¸í„°í˜ì´ìŠ¤ (ì¶”í›„ êµ¬í˜„)"""
    
    def _initialize_model(self):
        # TODO: OpenAI API ì´ˆê¸°í™”
        raise NotImplementedError("OpenAI ì¸í„°í˜ì´ìŠ¤ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def _generate_text(self, prompt: str) -> str:
        # TODO: OpenAI API í˜¸ì¶œ
        raise NotImplementedError("OpenAI ì¸í„°í˜ì´ìŠ¤ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# =============================================================================
# Claude êµ¬í˜„ (ì¶”í›„ í™•ì¥ìš©)
# =============================================================================

class ClaudeInterface(LLMInterface):
    """Anthropic Claude API ì¸í„°í˜ì´ìŠ¤ (ì¶”í›„ êµ¬í˜„)"""
    
    def _initialize_model(self):
        # TODO: Claude API ì´ˆê¸°í™”
        raise NotImplementedError("Claude ì¸í„°í˜ì´ìŠ¤ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def _generate_text(self, prompt: str) -> str:
        # TODO: Claude API í˜¸ì¶œ
        raise NotImplementedError("Claude ì¸í„°í˜ì´ìŠ¤ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# =============================================================================
# íŒ©í† ë¦¬ í•¨ìˆ˜
# =============================================================================

def create_llm_interface() -> LLMInterface:
    """í˜„ì¬ ì„¤ì •ì— ë”°ë¥¸ LLM ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
    config = get_current_llm_config()
    
    if LLM_PROVIDER == "gemini":
        return GeminiInterface(config)
    elif LLM_PROVIDER == "openai":
        return OpenAIInterface(config)
    elif LLM_PROVIDER == "claude":
        return ClaudeInterface(config)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM ì œê³µì: {LLM_PROVIDER}")

# =============================================================================
# í¸ì˜ í•¨ìˆ˜
# =============================================================================

class InsightGenerator:
    """ì¸ì‚¬ì´íŠ¸ ìƒì„± í¸ì˜ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.llm = create_llm_interface()
    
    @error_handler
    def generate_data_summary(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ìš”ì•½ ìƒì„±"""
        return self.llm.generate_insight(analysis_results, "data_summary")
    
    @error_handler
    def generate_performance_analysis(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¶„ì„ ìƒì„±"""
        return self.llm.generate_insight(analysis_results, "performance_analysis")
    
    @error_handler
    def generate_efficiency_diagnosis(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """íš¨ìœ¨ì„± ì§„ë‹¨ ìƒì„±"""
        return self.llm.generate_insight(analysis_results, "efficiency_diagnosis")
    
    @error_handler
    def generate_improvement_recommendations(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ê°œì„  ë°©ì•ˆ ìƒì„±"""
        return self.llm.generate_insight(analysis_results, "improvement_recommendations")
    
    @error_handler
    def generate_comprehensive_report(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        return self.llm.generate_insight(analysis_results, "comprehensive_report")
    
    def generate_all_insights(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë“  íƒ€ì…ì˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        all_insights = {}
        
        insight_types = [
            ("data_summary", self.generate_data_summary),
            ("performance_analysis", self.generate_performance_analysis),
            ("efficiency_diagnosis", self.generate_efficiency_diagnosis),
            ("improvement_recommendations", self.generate_improvement_recommendations),
            ("comprehensive_report", self.generate_comprehensive_report)
        ]
        
        for insight_type, generator_func in insight_types:
            try:
                result = generator_func(analysis_results)
                all_insights[insight_type] = result
                logger.info(f"{insight_type} ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"{insight_type} ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
                all_insights[insight_type] = {
                    "success": False,
                    "error": str(e),
                    "insight_type": insight_type
                }
        
        return all_insights

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    try:
        generator = InsightGenerator()
        print(f"âœ… LLM ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì„±ê³µ: {LLM_PROVIDER}")
        print(f"ëª¨ë¸: {generator.llm.config['model_name']}")
    except Exception as e:
        print(f"âŒ LLM ì¸í„°í˜ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        print("API í‚¤ì™€ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")